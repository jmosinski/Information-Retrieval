{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.026047,
     "end_time": "2021-04-07T16:48:11.364410",
     "exception": false,
     "start_time": "2021-04-07T16:48:11.338363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "from metrics import average_precision_score, norm_disc_cum_gain_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.033807,
     "end_time": "2021-04-07T16:48:11.457563",
     "exception": false,
     "start_time": "2021-04-07T16:48:11.423756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "    \n",
    "    def fit(self, x, y, lr=0.1, tol=1e-4):\n",
    "        # Initialize params\n",
    "        if self.w is None:\n",
    "            self.initialize(x.shape[1])\n",
    "        \n",
    "        # Optimize\n",
    "        prev_loss = np.inf\n",
    "        h = self.predict(x)\n",
    "        loss = self.loss(y, h)\n",
    "        i = 0\n",
    "        while np.abs(loss-prev_loss) > tol:\n",
    "            i += 1\n",
    "            prev_loss = loss\n",
    "            self.optim_step(lr, x, h, y)\n",
    "            h = self.predict(x)\n",
    "            loss = self.loss(y, h)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.sigmoid(x@self.w + self.b)\n",
    "    \n",
    "    def initialize(self, in_features):\n",
    "        self.w = 0.1 * np.random.rand(in_features)\n",
    "        self.b = 0\n",
    "        \n",
    "    def optim_step(self, lr, x, h, y):\n",
    "        err = h - y\n",
    "        self.w -= lr * np.mean(err*x.T, axis=1)\n",
    "        self.b -= lr * np.mean(err)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        '''Stable sigmoid implementation'''\n",
    "        \n",
    "        result = np.zeros_like(x)\n",
    "        \n",
    "        pos_mask = x >= 0\n",
    "        result[pos_mask] = 1 / (1+np.exp(-x[pos_mask]))\n",
    "        \n",
    "        neg_mask = ~pos_mask\n",
    "        exp_x = np.exp(x[neg_mask])\n",
    "        result[neg_mask] = exp_x / (1+exp_x)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def loss(y, h):\n",
    "        eps = 1e-100  # add small constant in logs to prevent numerical issues\n",
    "        return -np.mean(y*np.log(h+eps) + (1-y)*np.log(1-h+eps))\n",
    "    \n",
    "    def save(self, file_name):\n",
    "        params = {'w':self.w, 'b':self.b}\n",
    "        np.savez_compressed(file_name, **params)\n",
    "        \n",
    "    def load(self, file_name):\n",
    "        params = np.load(file_name)\n",
    "        self.w = params['w']\n",
    "        self.b = params['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.028182,
     "end_time": "2021-04-07T16:48:11.500564",
     "exception": false,
     "start_time": "2021-04-07T16:48:11.472382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BatchGenerator:\n",
    "    '''Batch Generator for doc ranking with sampling'''\n",
    "    \n",
    "    def __init__(self, x, y, batch_size=64, ratio=0.5, features_processor=lambda x: x):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.pos_rows, = np.where(y > 0)\n",
    "        self.neg_rows, = np.where(y == 0)\n",
    "        \n",
    "        self.pos_samples = int(batch_size * ratio)\n",
    "        self.neg_samples = batch_size - self.pos_samples\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.ratio = ratio  # frequency of positive samples\n",
    "        \n",
    "        self.features_processor = features_processor\n",
    "        \n",
    "    def get_batch(self):\n",
    "        pos_rows = np.random.choice(self.pos_rows, self.pos_samples)\n",
    "        neg_rows = np.random.choice(self.neg_rows, self.neg_samples)\n",
    "        samples = np.append(pos_rows, neg_rows)\n",
    "        np.random.shuffle(samples)\n",
    "        return self.features_processor(self.x[samples]), self.y[samples]\n",
    "    \n",
    "    def get_batches(self, n):\n",
    "        return [self.get_batch() for _ in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.026959,
     "end_time": "2021-04-07T16:48:11.542510",
     "exception": false,
     "start_time": "2021-04-07T16:48:11.515551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(model, data, feature_processor=lambda x: x):\n",
    "    '''Helper function used for model evaluation'''\n",
    "    \n",
    "    qids = list(data.keys())\n",
    "    qids = [int(qid) for qid in qids]\n",
    "    ndcg, ap = [], []\n",
    "    for qid in qids:\n",
    "        data = test_data[str(qid)]\n",
    "        pids, rels, x = data[:,0], data[:,1], data[:,2:]\n",
    "        x = feature_processor(x)\n",
    "        scores = model.predict(x)\n",
    "        idxs = np.argsort(-scores)\n",
    "        pids, scores, rels = pids[idxs], scores[idxs], rels[idxs]\n",
    "        ap.append(average_precision_score(rels))\n",
    "        ndcg.append(norm_disc_cum_gain_score(rels, k=100))\n",
    "    mean_ap = sum(ap) / len(ap)\n",
    "    mean_ndcg = sum(ndcg) / len(ndcg)\n",
    "    return mean_ap, mean_ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 29.550047,
     "end_time": "2021-04-07T16:48:41.107552",
     "exception": false,
     "start_time": "2021-04-07T16:48:11.557505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = np.load('./data/data_train.npz')['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.029362,
     "end_time": "2021-04-07T16:48:41.153912",
     "exception": false,
     "start_time": "2021-04-07T16:48:41.124550",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train, y_train = train_data[:,3:], train_data[:,2]\n",
    "del train_data  # Free memory\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015336,
     "end_time": "2021-04-07T16:48:41.185531",
     "exception": false,
     "start_time": "2021-04-07T16:48:41.170195",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Learning Rate Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.023708,
     "end_time": "2021-04-07T16:48:41.225511",
     "exception": false,
     "start_time": "2021-04-07T16:48:41.201803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "n_batches = 5000\n",
    "generator = BatchGenerator(x_train, y_train, batch_size=64, ratio=0.5)\n",
    "batches = generator.get_batches(n_batches)\n",
    "\n",
    "losses_dict = {}\n",
    "for lr in [0.1, 0.01, 0.001]:\n",
    "    model = LogisticRegression()\n",
    "    model.initialize(in_features=x_train.shape[-1])\n",
    "    losses = []\n",
    "    for epoch in range(1, epochs+1):\n",
    "        loss = 0\n",
    "        for x_batch, y_batch in batches:\n",
    "                preds = model.predict(x_batch)\n",
    "                model.optim_step(lr, x_batch, preds, y_batch)\n",
    "                loss += model.loss(y_batch, preds)\n",
    "        losses.append(loss / n_batches)\n",
    "    \n",
    "    plt.plot(range(1,len(losses)+1), losses, label=f'lr: {lr}')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('training loss')\n",
    "plt.legend()\n",
    "plt.savefig('lr_study.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015876,
     "end_time": "2021-04-07T16:48:41.257829",
     "exception": false,
     "start_time": "2021-04-07T16:48:41.241953",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model1 - concatenated query and passage embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 170.633758,
     "end_time": "2021-04-07T16:51:31.907555",
     "exception": false,
     "start_time": "2021-04-07T16:48:41.273797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "n_batches = 5000\n",
    "generator = BatchGenerator(x_train, y_train, batch_size=64, ratio=0.5)\n",
    "lr_schedule = lambda epoch: 0.01 * 0.98**(epoch-1)\n",
    "\n",
    "model1 = LogisticRegression()\n",
    "model1.initialize(in_features=x_train.shape[-1])\n",
    "\n",
    "losses = []\n",
    "batches = generator.get_batches(n_batches)\n",
    "for epoch in range(1, epochs+1):\n",
    "    lr = lr_schedule(epoch)\n",
    "    loss = 0\n",
    "    for x_batch, y_batch in batches:\n",
    "            preds = model1.predict(x_batch)\n",
    "            model1.optim_step(lr, x_batch, preds, y_batch)\n",
    "            loss += model1.loss(y_batch, preds)\n",
    "    losses.append(loss / n_batches)\n",
    "    if epoch%10 == 0:\n",
    "        print(f'Epoch: {epoch}, Loss: {losses[-1]}')\n",
    "\n",
    "losses1 = losses.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model2 - features: concatenated query and passage embeddings, absolute element-wise difference, element-wise product and cosine similarity. Inspired by InferSent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.031677,
     "end_time": "2021-04-07T16:51:31.959245",
     "exception": false,
     "start_time": "2021-04-07T16:51:31.927568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(x, z):\n",
    "    x_norm = np.linalg.norm(x, axis=1)\n",
    "    z_norm = np.linalg.norm(z, axis=1)\n",
    "    return np.sum(x*z, axis=1) / x_norm / z_norm\n",
    "\n",
    "def create_features(x):\n",
    "    idx = x.shape[-1] // 2\n",
    "    q_emb, p_emb = x[:,:idx], x[:,idx:]\n",
    "    abs_diff = np.abs(q_emb-p_emb)\n",
    "    prod = q_emb * p_emb\n",
    "    cos_sim = cosine_similarity(q_emb, p_emb).reshape(-1, 1)\n",
    "    return np.hstack((x, abs_diff, prod, cos_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 244.482898,
     "end_time": "2021-04-07T16:55:36.462296",
     "exception": false,
     "start_time": "2021-04-07T16:51:31.979398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "n_batches = 5000\n",
    "generator = BatchGenerator(x_train, y_train, batch_size=64,\n",
    "                           ratio=0.5, features_processor=create_features)\n",
    "lr_schedule = lambda epoch: 0.01 * 0.98**(epoch-1)\n",
    "\n",
    "model2 = LogisticRegression()\n",
    "in_features = x_train.shape[-1]//2*4 + 1\n",
    "model2.initialize(in_features)\n",
    "\n",
    "losses = []\n",
    "batches = generator.get_batches(n_batches)\n",
    "for epoch in range(1, epochs+1):\n",
    "    lr = lr_schedule(epoch)\n",
    "    loss = 0\n",
    "    for x_batch, y_batch in batches:\n",
    "        preds = model2.predict(x_batch)\n",
    "        model2.optim_step(lr, x_batch, preds, y_batch)\n",
    "        loss += model2.loss(y_batch, preds)\n",
    "    losses.append(loss / n_batches)\n",
    "    if epoch%10 == 0:\n",
    "        print(f'Epoch: {epoch}, Loss: {losses[-1]}')\n",
    "\n",
    "losses2 = losses.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023711,
     "end_time": "2021-04-07T16:55:36.510243",
     "exception": false,
     "start_time": "2021-04-07T16:55:36.486532",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Comapre models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.412265,
     "end_time": "2021-04-07T16:55:36.946484",
     "exception": false,
     "start_time": "2021-04-07T16:55:36.534219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(range(1,len(losses1)+1), losses1, label='Basic Features')\n",
    "plt.plot(range(1,len(losses2)+1), losses2, label='Additional Features')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('training loss')\n",
    "plt.legend()\n",
    "plt.savefig('LR_features.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.114446,
     "end_time": "2021-04-07T16:55:37.086289",
     "exception": false,
     "start_time": "2021-04-07T16:55:36.971843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "del x_train, y_train\n",
    "test_data = np.load('./data/data_val.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 50.821853,
     "end_time": "2021-04-07T16:56:27.934389",
     "exception": false,
     "start_time": "2021-04-07T16:55:37.112536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_ap, mean_ndcg = test(model1, test_data)\n",
    "print(f'Mean AP: {mean_ap}')  # 0.021492473677600318\n",
    "print(f'Mean nDCG: {mean_ndcg}')  # 0.05529640685379329"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 45.930844,
     "end_time": "2021-04-07T16:57:13.906666",
     "exception": false,
     "start_time": "2021-04-07T16:56:27.975822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_ap, mean_ndcg = test(model2, test_data, create_features)\n",
    "print(f'Mean AP: {mean_ap}')  # 0.030926104220628112\n",
    "print(f'Mean nDCG: {mean_ndcg}')  # 0.08210214768471422"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.038087,
     "end_time": "2021-04-07T16:57:13.992766",
     "exception": false,
     "start_time": "2021-04-07T16:57:13.954679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Best model\n",
    "model = model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.027062,
     "end_time": "2021-04-07T16:57:14.047141",
     "exception": false,
     "start_time": "2021-04-07T16:57:14.020079",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 46.25651,
     "end_time": "2021-04-07T16:58:00.330485",
     "exception": false,
     "start_time": "2021-04-07T16:57:14.073975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qids = list(test_data.keys())\n",
    "qids = [int(qid) for qid in qids]\n",
    "ndcg, ap = [], []\n",
    "with open('LR.txt', 'w') as f:\n",
    "    for qid in qids:\n",
    "        data = test_data[str(qid)]\n",
    "        pids, rels, x = data[:,0], data[:,1], data[:,2:]\n",
    "        x = create_features(x)\n",
    "        scores = model.predict(x)\n",
    "        idxs = np.argsort(-scores)\n",
    "        pids, scores, rels = pids[idxs], scores[idxs], rels[idxs]\n",
    "        ap.append(average_precision_score(rels))\n",
    "        ndcg.append(norm_disc_cum_gain_score(rels, k=100))\n",
    "        for i in range(scores[:100].size):\n",
    "            rank = i+1\n",
    "            f.write(f'{qid} A1 {pids[i]} {rank} {scores[i]} LR\\n')\n",
    "\n",
    "print(f'Mean AP: {sum(ap) / len(ap)}')  # 0.030926104220628112\n",
    "print(f'Mean nDCG: {sum(ndcg) / len(ndcg)}')  # 0.08210214768471422"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 596.754089,
   "end_time": "2021-04-07T16:58:01.192290",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-04-07T16:48:04.438201",
   "version": "2.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
