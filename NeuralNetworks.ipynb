{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "from metrics import average_precision_score, norm_disc_cum_gain_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseScoreNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.module = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.module(x)\n",
    "\n",
    "\n",
    "class Batcher:\n",
    "    \"\"\"Batch Generator for doc ranking with negative sampling\"\"\"\n",
    "    \n",
    "    def __init__(self, x, y, batch_size=16, ratio=0.5):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.pos_rows, = np.where(y > 0)\n",
    "        self.neg_rows, = np.where(y == 0)\n",
    "        \n",
    "        self.pos_samples = int(batch_size * ratio)\n",
    "        self.neg_samples = batch_size - self.pos_samples\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.ratio = ratio\n",
    "        \n",
    "    def get_batch(self):\n",
    "        pos_rows = np.random.choice(self.pos_rows, self.pos_samples)\n",
    "        neg_rows = np.random.choice(self.neg_rows, self.neg_samples)\n",
    "        samples = np.append(pos_rows, neg_rows)\n",
    "        np.random.shuffle(samples)\n",
    "        x = torch.tensor(self.x[samples], dtype=torch.float32, device=device)\n",
    "        y = torch.tensor(self.y[samples].reshape(-1,1), dtype=torch.float32, device=device)\n",
    "        return x, y\n",
    "    \n",
    "    def get_batches(self, n):\n",
    "        return [self.get_batch() for _ in range(n)]\n",
    "\n",
    "\n",
    "def train(model, batches, optimizer, criterion, epochs=1):\n",
    "    n = len(batches)\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        correct = 0\n",
    "        for x_batch, y_batch in batches:\n",
    "            loss, output = take_opt_step(model, x_batch, y_batch, optimizer, criterion)\n",
    "            epoch_loss += loss\n",
    "            correct += (torch.round(output)==y_batch).sum().item() / y_batch.shape[0]\n",
    "        acc = correct / n\n",
    "        accuracies.append(acc)\n",
    "        losses.append(epoch_loss)\n",
    "        print(f'Epoch: {epoch+1}, loss: {epoch_loss}, acc: {acc}')\n",
    "    return losses, accuracies\n",
    "\n",
    "def take_opt_step(model, x, y, optimizer, criterion):\n",
    "    model.zero_grad()\n",
    "    output = model(x)\n",
    "    loss = criterion(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item(), output\n",
    "\n",
    "def test_dense(model, file_name):\n",
    "    test_data = np.load('../input/irdm-data/val_data.npz')\n",
    "\n",
    "    qids = list(test_data.keys())\n",
    "    qids = [int(qid) for qid in qids]\n",
    "    ndcg, ap = [], []\n",
    "    with open(file_name, 'w') as f:\n",
    "        for qid in qids:\n",
    "            data = test_data[str(qid)]\n",
    "            pids, rels, x = data[:,0], data[:,1], torch.tensor(data[:,2:], dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                scores = model(x).cpu().numpy().reshape(-1)\n",
    "            idxs = np.argsort(-scores)\n",
    "            pids, scores, rels = pids[idxs], scores[idxs], rels[idxs]\n",
    "            ap.append(average_precision_score(rels))\n",
    "            ndcg.append(norm_disc_cum_gain_score(rels, k=100))\n",
    "            for i in range(scores[:100].size):\n",
    "                rank = i+1\n",
    "                f.write(f'{qid} A1 {pids[i]} {rank} {scores[i]} NN\\n')\n",
    "\n",
    "    print(f'Mean AP: {sum(ap) / len(ap)}')\n",
    "    print(f'Mean nDCG: {sum(ndcg) / len(ndcg)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = np.load('../input/irdm-data/train_data.npz')['arr_0']\n",
    "x, y = data_train[:,3:], data_train[:,2]\n",
    "del data_train\n",
    "batches = Batcher(x, y).get_batches(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DenseScoreNet().train().to(device)\n",
    "losses, accuracies = train(\n",
    "    model, batches,\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3),\n",
    "    criterion = nn.BCELoss(),\n",
    "    epochs = 10,\n",
    ")\n",
    "model = model.eval()\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dense(model, 'NN_ds.txt')\n",
    "# Mean AP: 0.030199232517960098\n",
    "# Mean nDCG: 0.08230779196166513"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Pairwise Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DensePairwiseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.subnet = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.subnet(x1)\n",
    "        x2 = self.subnet(x2)\n",
    "        return self.sigmoid(x1-x2)\n",
    "    \n",
    "    \n",
    "class PairwiseBatcher:\n",
    "    def __init__(self, data_dict, is_text=True):\n",
    "        self.data = data_dict\n",
    "        self.n_queries = len(data_dict)\n",
    "        self.is_text = is_text\n",
    "        \n",
    "    def get_batch(self, batch_size=16):\n",
    "        qids = np.random.choice(list(self.data.keys()), size=batch_size)\n",
    "        batch = []\n",
    "        x1, x2, y = [], [], []\n",
    "        for qid in qids:\n",
    "            nrel, rel = self.data[qid] \n",
    "            if len(rel)==0 or len(nrel)==0: continue\n",
    "            rel = rel[np.random.randint(len(rel))]\n",
    "            nrel = nrel[np.random.randint(len(nrel))]\n",
    "            if np.random.choice([True, False]):\n",
    "                x1.append(rel)\n",
    "                x2.append(nrel)\n",
    "                y.append(1)\n",
    "            else:\n",
    "                x1.append(nrel)\n",
    "                x2.append(rel)\n",
    "                y.append(0)\n",
    "        \n",
    "        if self.is_text:\n",
    "            return x1, x2, torch.tensor(y, dtype=torch.float32, device=device)\n",
    "        else:\n",
    "            return (torch.tensor(x1, dtype=torch.float32, device=device),\n",
    "                    torch.tensor(x2, dtype=torch.float32, device=device),\n",
    "                    torch.tensor(y, dtype=torch.float32, device=device))\n",
    "\n",
    "    def get_batches(self, n_batches, batch_size=16):\n",
    "        return [self.get_batch(batch_size) for _ in range(n_batches)]\n",
    "    \n",
    "    \n",
    "def train_pairwise(model, batches, epochs, lr):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    losses = []\n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_losses = []\n",
    "        for x1, x2, y in batches:\n",
    "            model.zero_grad()\n",
    "            output = model(x1, x2)\n",
    "            loss = criterion(output, y.reshape(-1,1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_losses.append(loss.item())\n",
    "        losses.append(sum(epoch_losses) / len(epoch_losses))\n",
    "        print(f'Epoch: {epoch}, Loss: {losses[-1]}')\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = np.load('../input/irdm-data/train_data.npz')['arr_0']\n",
    "\n",
    "data_dict = defaultdict(lambda:[[],[]])\n",
    "for row in data_train:\n",
    "    qid, rel, x = int(row[0]), int(row[2]), row[3:]\n",
    "    if rel == 0:\n",
    "        data_dict[qid][0].append(x)\n",
    "    else:\n",
    "        data_dict[qid][1].append(x)\n",
    "        \n",
    "batches = PairwiseBatcher(data_dict, is_text=False).get_batches(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DensePairwiseNet().train().to(device)\n",
    "losses = train_pairwise(model, batches, epochs=10, lr=1e-3)\n",
    "model = model.eval().subnet\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dense(model, 'NN_dp.txt')\n",
    "# Mean AP: 0.033328058396676935\n",
    "# Mean nDCG: 0.08553910330324341"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Embeddings with Pairwise Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseBertNet(nn.Module):\n",
    "    def __init__(self, embedder):\n",
    "        super().__init__()\n",
    "        self.embedder = embedder\n",
    "        self.module = nn.Sequential(\n",
    "            nn.Linear(2*768, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        q, p = [], []\n",
    "        for xi in x:\n",
    "            q.append(xi[0])\n",
    "            p.append(xi[1])\n",
    "        q = self.embedder.encode(q, convert_to_tensor=True,\n",
    "                                 show_progress_bar=False)\n",
    "        p = self.embedder.encode(p, convert_to_tensor=True,\n",
    "                                 show_progress_bar=False)\n",
    "        x = torch.hstack((q, p))\n",
    "        return self.module(x)\n",
    "\n",
    "    \n",
    "class BertPairwiseNet(nn.Module):\n",
    "    def __init__(self, embedder):\n",
    "        super().__init__()\n",
    "        self.subnet = BaseBertNet(embedder)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.subnet(x1)\n",
    "        x2 = self.subnet(x2)\n",
    "        return self.sigmoid(x1-x2)\n",
    "    \n",
    "    \n",
    "def test_bert(model, file_name, has_predict=False):\n",
    "    val_df = pd.read_table('../input/irdmdata/validation_data.tsv', sep='\\t')\n",
    "\n",
    "    ndcg, ap = [], []\n",
    "    with open(file_name, 'w') as f:\n",
    "        for qid in val_df.qid.unique():\n",
    "            q_df = val_df[val_df.qid==qid]\n",
    "            data = q_df.apply(lambda x: (x.queries, x.passage), axis=1)\n",
    "            if has_predict:\n",
    "                scores = model.predict(list(data), show_progress_bar=False)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    scores = model(list(data)).cpu().numpy().reshape(-1)\n",
    "            idxs = np.argsort(-scores)\n",
    "            scores = scores[idxs]\n",
    "            pids = q_df.pid.values[idxs]\n",
    "            rels = q_df.relevancy.values[idxs]\n",
    "            ap.append(average_precision_score(rels))\n",
    "            ndcg.append(norm_disc_cum_gain_score(rels, k=100))\n",
    "            for i in range(scores[:100].size):\n",
    "                rank = i+1\n",
    "                f.write(f'{qid} A1 {pids[i]} {rank} {scores[i]} NN\\n')\n",
    "\n",
    "    print(f'Mean AP: {sum(ap) / len(ap)}')\n",
    "    print(f'Mean nDCG: {sum(ndcg) / len(ndcg)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning Bert embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_bert_dataset():\n",
    "    train_df = pd.read_table('../input/irdmdata/train_data.tsv', sep='\\t')    \n",
    "    mask = (train_df['relevancy']==1).values\n",
    "    idxs = np.random.choice(np.arange(mask.size), size=mask.sum(), replace=False)\n",
    "    mask[idxs] = True\n",
    "    train_df = train_df[mask]\n",
    "    \n",
    "    dataset = []\n",
    "    for qid in train_df.qid.unique():\n",
    "        q_df = train_df[train_df.qid==qid]\n",
    "        data = q_df.apply(lambda x: InputExample(texts=[x.queries, x.passage],\n",
    "                                                 label=x.relevancy), axis=1)\n",
    "        dataset += list(data)\n",
    "    \n",
    "    return  DataLoader(dataset, shuffle=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer('msmarco-distilbert-base-v3', device=device)\n",
    "dataset = get_bert_dataset()\n",
    "loss = losses.CosineSimilarityLoss(embedder)\n",
    "embedder.fit(\n",
    "    train_objectives = [(dataset, loss)],\n",
    "    epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the ranking model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_table('../input/irdmdata/train_data.tsv', sep='\\t') \n",
    "mask = (train_df['relevancy']==1).values\n",
    "idxs = np.random.choice(np.arange(mask.size), size=mask.size//10, replace=False)\n",
    "mask[idxs] = True\n",
    "train_df = train_df[mask]\n",
    "\n",
    "data_dict = defaultdict(lambda:[[],[]])\n",
    "for i, row in train_df.iterrows():\n",
    "    qid, rel, q, p = row.qid, row.relevancy, row.queries, row.passage\n",
    "    if rel == 0:\n",
    "        data_dict[qid][0].append((q, p))\n",
    "    else:\n",
    "        data_dict[qid][1].append((q, p))\n",
    "\n",
    "del train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = PairwiseBatcher(data_dict).get_batches(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertPairwiseNet(embedder).train().to(device)\n",
    "losses = train_pairwise(model, batches, epochs=8, lr=1e-3)\n",
    "model = model.eval().subnet\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bert(model, 'NN_b.txt')\n",
    "# Mean AP: 0.22555985292329858\n",
    "# Mean nDCG: 0.3447011450899185"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Cross Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "model = CrossEncoder('cross-encoder/ms-marco-TinyBERT-L-2', device=device)\n",
    "dataset = get_bert_dataset()\n",
    "model.fit(\n",
    "    train_dataloader=dataset,\n",
    "    epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bert(model, 'NN.txt', has_predict=True)\n",
    "# Mean AP: 0.3683061733102693\n",
    "# Mean nDCG: 0.4914604699183384"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
